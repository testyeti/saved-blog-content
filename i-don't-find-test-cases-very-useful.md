## I don't find test-cases very useful
source: https://always-fearful.blogspot.co.uk/2017/06/i-dont-find-test-cases-very-useful.html by Amit 27/07/2017

Well, Randall Munroe (the XKCD guy) has it right. Someone wrong on the internet is a great motivator. I was flipping through my reading list, when I encountered an article with the oh-so-daring title "You Don’t Have Test Cases, Think Again". Well - The people at PractiTest blog usually write deep thoughtful posts, so there is probably a catch hiding in the post that will lead to an interesting thought trail. Also, reading through my phone, I casually skipped the warning notice at the top of the page and went on reading. As I read through, it became clear to me that I completely disagree with the content there. Hence - this post here.

Probably the easiest thing would be to be the friendly neighborhood bully and pick up on some of the many mistakes there are in the post that indicates that the author either has no understanding of what exploratory tests are, or  he is using a very private and skewed definition of ET (some candidates are the claim that there is no test planning in ET, the axiom "good tests have well defined and agreed upon pass criteria in advance" or my favorite: "[...]some Exploratory gurus assiduously disdain requirements; so they’re very unlikely to get involved with intermediate development deliverables prior to executable code"), but I prefer to avoid public ranting and patronizing people I don't know, so instead, I'd rather say why I don't think in terms of test cases and why I usually avoid this language.

I'll start with the obvious truth - some of what I do can be described in test-cases language. While I'm "testing" (or "exploring", if you prefer this term) I'm doing a lot of checking that can be translated to test-cases language, as can some of my more vague observations (e.g. "this doesn't look comfortable") could sometimes be translated to a test case without stretching it (in fact, once a problem is spotted, defining a test case around it is usually easy). So, yes - if someone insists, I am "executing test-cases" and I'm doing it wrong in as many ways as shorthand writing is doing calligraphy wrong.
And that's, in short, is the reason I try to avoid using the language of "test cases" - It comes with a lot's of baggage. When I say "test case" I'm also saying "heavy predefined documents" and "step-by-step instructions", which is not what I'm trying to convey. When I talk about my testing, I focus on getting my team to agree on what we should test, on what risks do we want to address and I rely on my skills. During my work I will use checklists, I will review any source of information I have (and get those I need) and have my team review both my intentions and my work. Heck, we've recently spent a full week reformatting data into excel sheet just so that we could deal with an unusually large testing problem. And yet - I don't say "test cases".

Another matter I want to address is the odd dichotomy created between "exploratory testing" and writing a test case. When you think about it closely, it is pretty much the same thing. When I spent some time writing down very detailed test cases, I could never do it without actually having the software and performing it at the same time, and I have yet to meet a tester who wrote a test plan without "winging it". Sure, some of us improvise magnificently using all sorts of techniques  - from equivalence partitioning to combinatorial testing to personas and use cases - but all of that charade is simply masking the fact that we apply our experience, skills and gut feelings to choosing the data and the techniques that will be applied (no, I'm not saying those techniques are a sophisticated fraud, they are valuable, very much so sometimes, but the skilled tester will know when to apply each technique or tool and when not to).
I want to ask you to think of the following question: what would be the result of exploratory testing a requirements document?

...

...

...

...

Done thinking? have your answer? Great. Unless, of course, your answer was "exploratory testing must have a working product to act upon", which is simply wrong. But in any other case, great. 
My answer is - test plan. When I test a requirements document (for those who use the ISTQB vocabulary, please recall that a review is a form of a static test), I come up with a model of how the software is supposed to function, with questions pointing at gaps in this model, or inconsistencies in the requirements, I compose a list of risks that we might want to put some safeguards against (some of them may be "test this area once it's done and look for that sort of glitches") and I add my knowledge and experience back to the team. I was definitely testing, and since I was learning about the product during this activity, I would say it falls under the definition of "exploratory" part as well (Also, if we avoid quoting anonymous "gurus" and stick with quotes we can trace, I tend to agree with Bach & Bolton's definition, according to which all testing is exploratory).
So, if we accept that writing a test plan is, in fact, an act of exploratory testing, why do we claim that executing the results is any different? And why do we insist on separating test execution from test planning? Sure, there are cases where we might want to plan more beforehand - to make sure we don't forget or to fight off some of the bias that we might have after testing the same thing for a while, but there are other cases where we might not want to do so. Relying on skilled testing allows us that freedom of doing what we think is right for our project. Which leads us back to why I don't like using the term "test case" - it just draws the focus from what's important.
